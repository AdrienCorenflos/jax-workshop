{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIFFERENTIABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will present the differences between forward and reverse mode automatic differentiation (AD) as well as the jax routines to retrieve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginner\n",
    "### Prerequisites\n",
    "Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, jvp, vjp, jacfwd, jacrev\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a \"canonical\" JAX example function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fun(x, y):\n",
    "    return x * jnp.sin(y) + jnp.cos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is less to present a comprehensive mathematical study of AD, and more to give an intuition behind the concepts that dictated JAX implementation. I'll be following this [presentation](https://www.youtube.com/watch?v=zqaJeKZXS1U), so you can find references embedded within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward mode differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a function $f \\colon \\mathbb{R}^n \\to  \\mathbb{R}^m$, provided it is differentiable at a point $x$, its directional Taylor expansion is given by:\n",
    "$$\n",
    "    f(x + \\epsilon y + o(\\epsilon)) = f(x) + \\epsilon J_x[f] y + o(\\epsilon)\n",
    "$$\n",
    "Let's define the concatenation of $x$ and of said \"dual\" number $\\vec{y}$: $x \\triangleright y$, then the Taylor identity can be written as\n",
    "$$\n",
    "    \\tilde{f}(x \\triangleright y) = f(x)\\triangleright J_x[f]y\n",
    "$$\n",
    "This directly expresses the forward chain rule as a composition of $\\tilde{f}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\tilde{f} \\circ \\tilde{g}(x \\triangleright y) \n",
    "        &= \\tilde{f}(g(x) \\triangleright J_x[g]y)\\\\\n",
    "        &= f \\circ g(x) \\triangleright J_{g(x)}J_x[g]y\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse mode differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other end, what can we say about $y^\\top J_x[f]$? The chain rule for this is now written\n",
    "$$\n",
    "    y^\\top J_x[f\\circ g] = (y^\\top J_{g(x)}[f]) J_{x}[g]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what can be seen now is that to compute the forward mode derivative, you need to \"carry\" your gradients as you compute the output, but for the backward mode you need to remember the intermediate values of the functions involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided the Jabobian don't depend on the dual number $y$, both operations are dual to each other, so that one can be retrieved from the other via a transposition (TODO I still need to understand the exact implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allowed JAX developers to implement a free conversion from JVP (easier to implement) to VJP rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice the complexity of each methods will depend on the input-output sizes: if $f \\colon \\mathbb{R}^n \\to  \\mathbb{R}^m$, its Jacobian will be in $\\mathbb{R}^{n \\times n}$ so that the forward operation will have complexity $O(n)$ and the backward one $O(m)$ with the additional constraint that the intermediate inputs will need to be remembered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of both methods actually follows directly from the derivation: the forward mode computes the output gradient directly, while the backward only returns a function that is capable of evaluating the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DeviceArray(1.2184019, dtype=float32), DeviceArray(0.56805766, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(jvp(my_fun, (0.5, 0.75), (1., 1.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2184019\n",
      "Partial(functools.partial(<function _vjp_pullback_wrapper at 0x7f5743ae98c0>, [dtype('float32')], (*, PyTreeDef(tuple, [*,*]))), Partial(functools.partial(<function vjp.<locals>.unbound_vjp at 0x7f573b0f2710>, [(ShapedArray(float32[]), *)], { lambda  ; c d.\n",
      "  let e = mul c 0.681638777256012\n",
      "      f = mul d 0.731688916683197\n",
      "      g = mul 0.5 f\n",
      "      h = add_any e g\n",
      "      i = mul c 0.4794255495071411\n",
      "      j = neg i\n",
      "      k = add_any h j\n",
      "  in (k,) }), ()))\n"
     ]
    }
   ],
   "source": [
    "val, my_fun_vjp = vjp(my_fun, 0.5, 0.75)\n",
    "print(val)\n",
    "print(my_fun_vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DeviceArray(0.20221323, dtype=float32), DeviceArray(0.36584446, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(my_fun_vjp(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence it is also possible to compute Jacobians, using one or the other method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.36204547, dtype=float32),\n",
       " DeviceArray(0.27015114, dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacfwd(my_fun, argnums=(0, 1))(0.5, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.36204547, dtype=float32),\n",
       " DeviceArray(0.27015114, dtype=float32))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacrev(my_fun, argnums=(0, 1))(0.5, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: \n",
    "Compute the derivative of the following $\\ln(1 + \\exp(x))$ at $x = 100$, what do you think is happening?\n",
    "```python\n",
    "def log1p(x):\n",
    "    return jnp.log(1 + jnp.exp(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "Compute the Hessian of the norm function: $(\\sum_{i=1}^n x_i^2)^{\\frac 1 2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate\n",
    "### Prerequisites\n",
    "- Beginner autodiff\n",
    "- Beginner loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from jax import custom_jvp, custom_jvp, jvp, vjp, make_jaxpr\n",
    "from jax.lax import scan, while_loop\n",
    "import jax.numpy as jax_np\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will understand why sometimes we need to implement our AD routines manually and how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take two different examples, the first one is the `log1p` from the beginner section, the second is the iterative implementation of the square root function in the controlflow.loops notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOG1P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will help us highlight some problems that can happen when relying on AD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "(DeviceArray(nan, dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "def log1p(x):\n",
    "    return 1. / jnp.log(1 + jnp.exp(x))\n",
    "\n",
    "val, log1p_vjp = vjp(log1p, 100.)\n",
    "print(val)\n",
    "print(log1p_vjp(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the derivative should be $0.$, so what's happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let b = mul a 0.0\n",
       "      c = mul b 1.0\n",
       "      d = neg c\n",
       "      e = div d inf\n",
       "      f = mul e inf\n",
       "  in (f,) }"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(log1p_vjp)(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh! It's an indefinite form $\\frac{\\inf}{\\inf}$, we need to resolve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "log1p = custom_jvp(log1p)\n",
    "@log1p.defjvp\n",
    "def _log1p_jvp(primals, tangents):\n",
    "    x, = primals\n",
    "    x_dot = tangents,\n",
    "    \n",
    "    ex = jnp.exp(x)\n",
    "    primal_out = 1. / jnp.log(1 + ex)\n",
    "    \n",
    "    ex_inv = 1 / ex\n",
    "    tangent_out = 1 / ((1 + ex_inv) * primal_out ** 2)\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "# otherwise: \n",
    "# log1p.defjvps(lambda x_dot, primal_out, x: ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "(array(0., dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "val, log1p_vjp = vjp(log1p, 100.)\n",
    "print(val)\n",
    "print(log1p_vjp(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the while loop implementation of the square root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def while_loop_sqrt(x, x0=1., n_iter=10):\n",
    "    \n",
    "    def cond(carry):\n",
    "        i, _val = carry\n",
    "        return i < n_iter\n",
    "    \n",
    "    def body(carry):\n",
    "        i, val = carry\n",
    "        return i+1, 0.5 * (val + x / val)\n",
    "    \n",
    "    _, res = while_loop(cond, body, (0,  x0))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-77-bc57d825fa89>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mval\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwhile_loop_sqrt_vjp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvjp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwhile_loop_sqrt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m100.\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwhile_loop_sqrt_vjp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1.\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/api.py\u001B[0m in \u001B[0;36m_vjp_pullback_wrapper\u001B[0;34m(cotangent_dtypes, io_tree, fun, py_args)\u001B[0m\n\u001B[1;32m   1791\u001B[0m              \"with dtype {}.\")\n\u001B[1;32m   1792\u001B[0m       \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mct_dtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexpected_tangent_dtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_dtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1793\u001B[0;31m   \u001B[0mans\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1794\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mtree_unflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout_tree\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mans\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1795\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/interpreters/ad.py\u001B[0m in \u001B[0;36munbound_vjp\u001B[0;34m(pvals, jaxpr, consts, *cts)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[0mcts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mignore_consts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m     \u001B[0mdummy_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mUndefinedPrimal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maval\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mjaxpr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minvars\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m     \u001B[0marg_cts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbackward_pass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjaxpr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconsts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdummy_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstantiate_zeros\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg_cts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/interpreters/ad.py\u001B[0m in \u001B[0;36mbackward_pass\u001B[0;34m(jaxpr, consts, primals_in, cotangents_in)\u001B[0m\n\u001B[1;32m    219\u001B[0m       \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m         cts_out = get_primitive_transpose(eqn.primitive)(cts_in, *invals,\n\u001B[0;32m--> 221\u001B[0;31m                                                          **eqn.params)\n\u001B[0m\u001B[1;32m    222\u001B[0m     \u001B[0mcts_out\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mZero\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maval\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0meqn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minvars\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mcts_out\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mZero\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mcts_out\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[0;31m# FIXME: Some invars correspond to primals!\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/_src/lax/control_flow.py\u001B[0m in \u001B[0;36m_while_transpose_error\u001B[0;34m(*_, **kwargs)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_while_transpose_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 538\u001B[0;31m   raise ValueError(\"Reverse-mode differentiation does not work for \"\n\u001B[0m\u001B[1;32m    539\u001B[0m                    \u001B[0;34m\"lax.while_loop or lax.fori_loop. \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    540\u001B[0m                    \"Try using lax.scan instead.\")\n",
      "\u001B[0;31mValueError\u001B[0m: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead."
     ]
    }
   ],
   "source": [
    "val, while_loop_sqrt_vjp = vjp(while_loop_sqrt, 100.)\n",
    "print(val)\n",
    "print(while_loop_sqrt_vjp(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can't use the backward-mode for a while loop, that's a bummer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the beginner section, reverse-mode needs to allocate memory, which can't be done dynamically in XLA, so that JAX doesn't allow for reverse mode differentiation through while_loops which could grow indefinitely. Instead some work has been planned in JAX to allow for bounded size while loops to be implemented using the scan syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(custom_jvp, nondiff_argnums=(1, 2))\n",
    "def custom_jvp_while_loop_sqrt(x, x0=1., n_iter=10):\n",
    "    sqrt_x = while_loop_sqrt(x, x0, n_iter)\n",
    "    return sqrt_x\n",
    "\n",
    "@custom_jvp_while_loop_sqrt.defjvp\n",
    "def _custom_jvp_while_loop_sqrt(x0, n_iter, primals, tangents):\n",
    "    x, = primals\n",
    "    x_dot, = tangents\n",
    "    sqrt_x = while_loop_sqrt(x, x0, n_iter)\n",
    "    return sqrt_x, x_dot / (2 * sqrt_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2.236068, dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_jvp_while_loop_sqrt(5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "(DeviceArray(0.05, dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "val, while_loop_sqrt_vjp = vjp(custom_jvp_while_loop_sqrt, 100.)\n",
    "print(val)\n",
    "print(while_loop_sqrt_vjp(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: \n",
    "Implement the VJP rule for `log1p`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "Can you think of other cases when it could be useful to implement custom gradient? Can the `sqrt` approach be generalised?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "### Prerequisites\n",
    "- Intermediate autodiff \n",
    "- Beginner randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we reproduce a subpart of the [FFJORD](https://arxiv.org/pdf/1810.01367.pdf) paper, which aimes at inferring an efficient way to estimate the trace of the Jacobian of a given bijective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import make_jaxpr, jvp, vjp, jit\n",
    "from jax.lax import associative_scan, scan\n",
    "from jax.random import normal, PRNGKey\n",
    "\n",
    "import jax.numpy as jax_np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first present the naive (and exact) way of computing the trace of a Jacobian using JAX primitives, then discuss the complexity of the operation and introduce the [Hutchinson trick](https://www.tandfonline.com/doi/abs/10.1080/03610919008812866) as a way to reduce it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5605685732924224\n"
     ]
    }
   ],
   "source": [
    "D = 5\n",
    "a = np.random.randn(D, D)\n",
    "b = np.random.randn(D)\n",
    "\n",
    "def affine_function(x):\n",
    "    return a @ x + b\n",
    "\n",
    "print(np.trace(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we compute the trace of the Jacobian using JVP or VJP operations only?\n",
    "Let $(e_i)_{i=1}^D$ be the canonical basis of $\\mathbb{R}^d$, then for any $M \\in \\mathbb{R}^{d \\times d}$,\n",
    "$$\n",
    "\\mathrm{tr}(M) = \\sum_{i=1}^d e_i^\\top M e_i\n",
    "$$\n",
    "Or tautologically (in terms of identity, not computation)\n",
    "$$\n",
    "\\mathrm{tr}(M) = \\mathrm{tr}(M I_d)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jac_trace(f, x):\n",
    "    d = x.shape[0]\n",
    "    eis = jax_np.eye(d)\n",
    "    val, temp = jvp(f, (x,), (eis,))\n",
    "    return jax_np.trace(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(-0.5605686, dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_jac_trace(affine_function, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has complexity $O(d^2)$ even though the JVP operation has complexity $O(d)$..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead one can use the Hutchinston trick:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{tr}(M) = \\mathbb{E}\\left[\\epsilon^\\top M \\epsilon\\right]$ where $\\epsilon$ is for instance normally distributed: $\\epsilon \\sim \\mathcal{N}(0, I_d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-workshop",
   "language": "python",
   "name": "jax-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}