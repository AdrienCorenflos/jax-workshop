{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIFFERENTIABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will present the differences between forward and reverse mode automatic differentiation (AD) as well as the jax routines to retrieve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginner\n",
    "### Prerequisites\n",
    "Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, jvp, vjp, jacfwd, jacrev\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a \"canonical\" JAX example function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fun(x, y):\n",
    "    return x * jnp.sin(y) + jnp.cos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is less to present a comprehensive mathematical study of AD, and more to give an intuition behind the concepts that dictated JAX implementation. I'll be following this [presentation](https://www.youtube.com/watch?v=zqaJeKZXS1U), so you can find references embedded within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward mode differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a function $f \\colon \\mathbb{R}^n \\to  \\mathbb{R}^m$, provided it is differentiable at a point $x$, its directional Taylor expansion is given by:\n",
    "$$\n",
    "    f(x + \\epsilon y + o(\\epsilon)) = f(x) + \\epsilon J_x[f] y + o(\\epsilon)\n",
    "$$\n",
    "Let's define the concatenation of $x$ and of said \"dual\" number $\\vec{y}$: $x \\triangleright y$, then the Taylor identity can be written as\n",
    "$$\n",
    "    \\tilde{f}(x \\triangleright y) = f(x)\\triangleright J_x[f]y\n",
    "$$\n",
    "This directly expresses the forward chain rule as a composition of $\\tilde{f}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\tilde{f} \\circ \\tilde{g}(x \\triangleright y) \n",
    "        &= \\tilde{f}(g(x) \\triangleright J_x[g]y)\\\\\n",
    "        &= f \\circ g(x) \\triangleright J_{g(x)}J_x[g]y\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse mode differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other end, what can we say about $y^\\top J_x[f]$? The chain rule for this is now written\n",
    "$$\n",
    "    y^\\top J_x[f\\circ g] = (y^\\top J_{g(x)}[f]) J_{x}[g]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what can be seen now is that to compute the forward mode derivative, you need to \"carry\" your gradients as you compute the output, but for the backward mode you need to remember the intermediate values of the functions involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided the Jabobian don't depend on the dual number $y$, both operations are dual to each other, so that one can be retrieved from the other via a transposition (TODO I still need to understand the exact implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allowed JAX developers to implement a free conversion from JVP (easier to implement) to VJP rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice the complexity of each methods will depend on the input-output sizes: if $f \\colon \\mathbb{R}^n \\to  \\mathbb{R}^m$, its Jacobian will be in $\\mathbb{R}^{n \\times n}$ so that the forward operation will have complexity $O(n)$ and the backward one $O(m)$ with the additional constraint that the intermediate inputs will need to be remembered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of both methods actually follows directly from the derivation: the forward mode computes the output gradient directly, while the backward only returns a function that is capable of evaluating the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DeviceArray(1.2184019, dtype=float32), DeviceArray(0.56805766, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(jvp(my_fun, (0.5, 0.75), (1., 1.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2184019\n",
      "Partial(functools.partial(<function _vjp_pullback_wrapper at 0x7f5743ae98c0>, [dtype('float32')], (*, PyTreeDef(tuple, [*,*]))), Partial(functools.partial(<function vjp.<locals>.unbound_vjp at 0x7f573b0f2710>, [(ShapedArray(float32[]), *)], { lambda  ; c d.\n",
      "  let e = mul c 0.681638777256012\n",
      "      f = mul d 0.731688916683197\n",
      "      g = mul 0.5 f\n",
      "      h = add_any e g\n",
      "      i = mul c 0.4794255495071411\n",
      "      j = neg i\n",
      "      k = add_any h j\n",
      "  in (k,) }), ()))\n"
     ]
    }
   ],
   "source": [
    "val, my_fun_vjp = vjp(my_fun, 0.5, 0.75)\n",
    "print(val)\n",
    "print(my_fun_vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DeviceArray(0.20221323, dtype=float32), DeviceArray(0.36584446, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(my_fun_vjp(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence it is also possible to compute Jacobians, using one or the other method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.36204547, dtype=float32),\n",
       " DeviceArray(0.27015114, dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacfwd(my_fun, argnums=(0, 1))(0.5, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.36204547, dtype=float32),\n",
       " DeviceArray(0.27015114, dtype=float32))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacrev(my_fun, argnums=(0, 1))(0.5, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: \n",
    "Compute the derivative of the following $\\ln(1 + \\exp(x))$ at $x = 100$, what do you think is happening?\n",
    "```python\n",
    "def log1p(x):\n",
    "    return jnp.log(1 + jnp.exp(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "Compute the Hessian of the norm function: $(\\sum_{i=1}^n x_i^2)^{\\frac 1 2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate\n",
    "### Prerequisites\n",
    "- Beginner autodiff\n",
    "- Beginner loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from jax import custom_jvp, custom_jvp, jvp, vjp, make_jaxpr\n",
    "from jax.lax import scan, while_loop\n",
    "import jax.numpy as jax_np\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will understand why sometimes we need to implement our AD routines manually and how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take two different examples, the first one is the `log1p` from the beginner section, the second is the iterative implementation of the square root function in the controlflow.loops notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOG1P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will help us highlight some problems that can happen when relying on AD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "(DeviceArray(nan, dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "def log1p(x):\n",
    "    return 1. / jnp.log(1 + jnp.exp(x))\n",
    "\n",
    "val, log1p_vjp = vjp(log1p, 100.)\n",
    "print(val)\n",
    "print(log1p_vjp(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the derivative should be $0.$, so what's happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let b = mul a 0.0\n",
       "      c = mul b 1.0\n",
       "      d = neg c\n",
       "      e = div d inf\n",
       "      f = mul e inf\n",
       "  in (f,) }"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(log1p_vjp)(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh! It's an indefinite form $\\frac{\\inf}{\\inf}$, we need to resolve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "log1p = custom_jvp(log1p)\n",
    "@log1p.defjvp\n",
    "def _log1p_jvp(primals, tangents):\n",
    "    x, = primals\n",
    "    x_dot = tangents,\n",
    "    \n",
    "    ex = jnp.exp(x)\n",
    "    primal_out = 1. / jnp.log(1 + ex)\n",
    "    \n",
    "    ex_inv = 1 / ex\n",
    "    tangent_out = 1 / ((1 + ex_inv) * primal_out ** 2)\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "# otherwise: \n",
    "# log1p.defjvps(lambda x_dot, primal_out, x: ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "(array(0., dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "val, log1p_vjp = vjp(log1p, 100.)\n",
    "print(val)\n",
    "print(log1p_vjp(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the while loop implementation of the square root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def while_loop_sqrt(x, x0=1., n_iter=10):\n",
    "    \n",
    "    def cond(carry):\n",
    "        i, _val = carry\n",
    "        return i < n_iter\n",
    "    \n",
    "    def body(carry):\n",
    "        i, val = carry\n",
    "        return i+1, 0.5 * (val + x / val)\n",
    "    \n",
    "    _, res = while_loop(cond, body, (0,  x0))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-bc57d825fa89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhile_loop_sqrt_vjp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhile_loop_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhile_loop_sqrt_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/api.py\u001b[0m in \u001b[0;36m_vjp_pullback_wrapper\u001b[0;34m(cotangent_dtypes, io_tree, fun, py_args)\u001b[0m\n\u001b[1;32m   1791\u001b[0m              \"with dtype {}.\")\n\u001b[1;32m   1792\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_tangent_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m   \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36munbound_vjp\u001b[0;34m(pvals, jaxpr, consts, *cts)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mcts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_consts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mdummy_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mUndefinedPrimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0marg_cts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstantiate_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_cts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(jaxpr, consts, primals_in, cotangents_in)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         cts_out = get_primitive_transpose(eqn.primitive)(cts_in, *invals,\n\u001b[0;32m--> 221\u001b[0;31m                                                          **eqn.params)\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcts_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mZero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvars\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcts_out\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mZero\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcts_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# FIXME: Some invars correspond to primals!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36m_while_transpose_error\u001b[0;34m(*_, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_while_transpose_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m   raise ValueError(\"Reverse-mode differentiation does not work for \"\n\u001b[0m\u001b[1;32m    539\u001b[0m                    \u001b[0;34m\"lax.while_loop or lax.fori_loop. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m                    \"Try using lax.scan instead.\")\n",
      "\u001b[0;31mValueError\u001b[0m: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead."
     ]
    }
   ],
   "source": [
    "val, while_loop_sqrt_vjp = vjp(while_loop_sqrt, 100.)\n",
    "print(val)\n",
    "print(while_loop_sqrt_vjp(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can't use the backward-mode for a while loop, that's a bummer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the beginner section, reverse-mode needs to allocate memory, which can't be done dynamically in XLA, so that JAX doesn't allow for reverse mode differentiation through while_loops which could grow indefinitely. Instead some work has been planned in JAX to allow for bounded size while loops to be implemented using the scan syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(custom_jvp, nondiff_argnums=(1, 2))\n",
    "def custom_jvp_while_loop_sqrt(x, x0=1., n_iter=10):\n",
    "    sqrt_x = while_loop_sqrt(x, x0, n_iter)\n",
    "    return sqrt_x\n",
    "\n",
    "@custom_jvp_while_loop_sqrt.defjvp\n",
    "def _custom_jvp_while_loop_sqrt(x0, n_iter, primals, tangents):\n",
    "    x, = primals\n",
    "    x_dot, = tangents\n",
    "    sqrt_x = while_loop_sqrt(x, x0, n_iter)\n",
    "    return sqrt_x, x_dot / (2 * sqrt_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2.236068, dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_jvp_while_loop_sqrt(5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "(DeviceArray(0.05, dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "val, while_loop_sqrt_vjp = vjp(custom_jvp_while_loop_sqrt, 100.)\n",
    "print(val)\n",
    "print(while_loop_sqrt_vjp(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: \n",
    "Implement the vjp rule for `log1p`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "Can you think of other cases when it could be useful to implement custom gradient? Can the `sqrt` approach be generalised?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "### Prerequisites\n",
    "- Intermediate loops "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import make_jaxpr, jvp, vjp, jit\n",
    "from jax.lax import associative_scan, scan\n",
    "\n",
    "import jax.numpy as jax_np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now present an additional primitive which can be useful when dealing with associative binary operations (such as summation): `associative_scan`, also known as [prefix sum](https://en.wikipedia.org/wiki/Prefix_sum). It consists in applying recursive operations to subsets of the inputs (divide and conquer strategy) instead of applying it sequentially. This has the benefit of being easily parallelisable and is natively implemented in JAX. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice it implements a parallelised version (see for example the [Wikipedia](https://en.wikipedia.org/wiki/Prefix_sum) article, or ask Fatemeh, she's an expert at it) of the following algorithm:\n",
    "```python\n",
    "def my_sequential_associative_scan(binary_op, xs):\n",
    "    res = np.copy(xs)\n",
    "    val = xs[0]\n",
    "    for i, x in enumerate(xs[1:]):\n",
    "        val = binary_op(val, x)\n",
    "        res[i+1] = val\n",
    "    return res\n",
    "```\n",
    "\n",
    "so that the cumulative sum would be for example written as \n",
    "```python\n",
    "my_sequential_associative_scan(lambda x, y: x + y, np.arange(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using JAX this would actually be written in the following wa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.  6. 10. 15. 21. 28. 36. 45.]\n"
     ]
    }
   ],
   "source": [
    "def associative_cumulative_sum(xs):\n",
    "    return associative_scan(lambda x, y: x + y, xs)\n",
    "\n",
    "print(associative_cumulative_sum(np.arange(10.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the code generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let b = slice[ limit_indices=(9,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(2,) ] a\n",
       "      c = slice[ limit_indices=(10,)\n",
       "                 start_indices=(1,)\n",
       "                 strides=(2,) ] a\n",
       "      d = add b c\n",
       "      e = slice[ limit_indices=(4,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(2,) ] d\n",
       "      f = slice[ limit_indices=(5,)\n",
       "                 start_indices=(1,)\n",
       "                 strides=(2,) ] d\n",
       "      g = add e f\n",
       "      h = slice[ limit_indices=(1,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(2,) ] g\n",
       "      i = slice[ limit_indices=(2,)\n",
       "                 start_indices=(1,)\n",
       "                 strides=(2,) ] g\n",
       "      j = add h i\n",
       "      k = slice[ limit_indices=(0,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] j\n",
       "      l = slice[ limit_indices=(2,)\n",
       "                 start_indices=(2,)\n",
       "                 strides=(2,) ] g\n",
       "      m = add k l\n",
       "      n = slice[ limit_indices=(1,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] g\n",
       "      o = concatenate[ dimension=0 ] n m\n",
       "      p = pad[ padding_config=((0, 1, 1),) ] o 0.0\n",
       "      q = pad[ padding_config=((1, 0, 1),) ] j 0.0\n",
       "      r = add p q\n",
       "      s = slice[ limit_indices=(5,)\n",
       "                 start_indices=(2,)\n",
       "                 strides=(2,) ] d\n",
       "      t = add r s\n",
       "      u = slice[ limit_indices=(1,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] d\n",
       "      v = concatenate[ dimension=0 ] u t\n",
       "      w = pad[ padding_config=((0, 0, 1),) ] v 0.0\n",
       "      x = pad[ padding_config=((1, 1, 1),) ] r 0.0\n",
       "      y = add w x\n",
       "      z = slice[ limit_indices=(4,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] y\n",
       "      ba = slice[ limit_indices=(10,)\n",
       "                  start_indices=(2,)\n",
       "                  strides=(2,) ] a\n",
       "      bb = add z ba\n",
       "      bc = slice[ limit_indices=(1,)\n",
       "                  start_indices=(0,)\n",
       "                  strides=(1,) ] a\n",
       "      bd = concatenate[ dimension=0 ] bc bb\n",
       "      be = pad[ padding_config=((0, 1, 1),) ] bd 0.0\n",
       "      bf = pad[ padding_config=((1, 0, 1),) ] y 0.0\n",
       "      bg = add be bf\n",
       "  in (bg,) }"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(associative_cumulative_sum)(np.arange(10.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks just as when we were doing python loops! Is it bad? No, because the depth of the generated graph will only grow in $\\log_2(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as for `scan`, it is closed under differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DeviceArray([ 0.,  1.,  3.,  6., 10.], dtype=float32), DeviceArray([1., 2., 3., 4., 5.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(jvp(associative_cumulative_sum, (np.arange(5.),), (np.ones(5),)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.  6. 10.]\n",
      "(DeviceArray([5., 4., 3., 2., 1.], dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "val, cumsum_bwd = vjp(associative_cumulative_sum, np.arange(5.))\n",
    "print(val)\n",
    "print(cumsum_bwd(np.ones(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: \n",
    "Compare the speed of the associative_scan and scan implementation of cumulative sum on GPU and CPU (use the device flag in the jit function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "Implements your own parallel version of `associative_scan` using jax primitives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-workshop",
   "language": "python",
   "name": "jax-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
