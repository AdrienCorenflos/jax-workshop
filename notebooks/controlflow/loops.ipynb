{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn the different ways to implement loops (with fixed size or dynamic size) using JAX primitives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginner\n",
    "### Prerequisites\n",
    "No prerequisite - (Beginner if-else is better though)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:28:10.811086Z",
     "start_time": "2024-11-27T17:28:10.808111Z"
    }
   },
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import make_jaxpr\n",
    "from jax.lax import fori_loop"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first give an example of how to compute the cumulative sum of the values in an array of floats using JAX:\n",
    "```python\n",
    "def my_cumsum(xs):\n",
    "    res = np.zeros_like(xs)\n",
    "    for i, x in enumerate(xs):\n",
    "        res[i] = x + res[i-1]\n",
    "    return res\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following array for our tests:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:26:03.687801Z",
     "start_time": "2024-11-27T17:26:03.685422Z"
    }
   },
   "source": [
    "arr = np.arange(0.0, 10.0)\n",
    "print(arr)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FORI_LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most natural syntax would seem to be to use the fori_loop: let's first have a look at what simply summing the array would look like:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:26:03.801039Z",
     "start_time": "2024-11-27T17:26:03.746350Z"
    }
   },
   "source": [
    "def naive_sum(xs):\n",
    "    n = xs.shape[0]\n",
    "    val = 0.0\n",
    "    for i in jnp.arange(n):\n",
    "        val = val + xs[i]\n",
    "    return val\n",
    "\n",
    "\n",
    "print(naive_sum(arr))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use this directly on a jax array then?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:26:03.910404Z",
     "start_time": "2024-11-27T17:26:03.838399Z"
    }
   },
   "source": [
    "naive_sum(jnp.asarray(arr))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(45., dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well yes! So why all the fuss? Surely we can just not care right? Let write down a (bad) equivalent in jax and check it out:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:26:03.958913Z",
     "start_time": "2024-11-27T17:26:03.931829Z"
    }
   },
   "source": [
    "def naive_fori_loop_sum(xs):\n",
    "    xs = jnp.asarray(xs)\n",
    "    n = xs.shape[0]\n",
    "\n",
    "    def body(i, val):\n",
    "        # i is the iteration step\n",
    "        # val is the running value\n",
    "        val = val + xs[i]\n",
    "        return val\n",
    "\n",
    "    res = fori_loop(\n",
    "        0,  # starting index\n",
    "        n,  # total number of iterations: the last index is n-1\n",
    "        body,  # the function iterated during the loop: res = body(n-1, body(n-2, body(n-3,...)))\n",
    "        init_val=0.0,  # the initial value for the loop\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "print(naive_fori_loop_sum(arr))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:26:04.030270Z",
     "start_time": "2024-11-27T17:26:04.012591Z"
    }
   },
   "source": [
    "make_jaxpr(naive_sum)(jnp.asarray(arr))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[10]. let\n",
       "    b:i32[10] = iota[dimension=0 dtype=int32 shape=(10,) sharding=None] \n",
       "    c:i32[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] b\n",
       "    d:i32[] = squeeze[dimensions=(0,)] c\n",
       "    e:bool[] = lt d 0\n",
       "    f:i32[] = add d 10\n",
       "    g:i32[] = select_n e d f\n",
       "    h:f32[1] = dynamic_slice[slice_sizes=(1,)] a g\n",
       "    i:f32[] = squeeze[dimensions=(0,)] h\n",
       "    j:f32[] = add 0.0 i\n",
       "    k:i32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] b\n",
       "    l:i32[] = squeeze[dimensions=(0,)] k\n",
       "    m:bool[] = lt l 0\n",
       "    n:i32[] = add l 10\n",
       "    o:i32[] = select_n m l n\n",
       "    p:f32[1] = dynamic_slice[slice_sizes=(1,)] a o\n",
       "    q:f32[] = squeeze[dimensions=(0,)] p\n",
       "    r:f32[] = add j q\n",
       "    s:i32[1] = slice[limit_indices=(3,) start_indices=(2,) strides=(1,)] b\n",
       "    t:i32[] = squeeze[dimensions=(0,)] s\n",
       "    u:bool[] = lt t 0\n",
       "    v:i32[] = add t 10\n",
       "    w:i32[] = select_n u t v\n",
       "    x:f32[1] = dynamic_slice[slice_sizes=(1,)] a w\n",
       "    y:f32[] = squeeze[dimensions=(0,)] x\n",
       "    z:f32[] = add r y\n",
       "    ba:i32[1] = slice[limit_indices=(4,) start_indices=(3,) strides=(1,)] b\n",
       "    bb:i32[] = squeeze[dimensions=(0,)] ba\n",
       "    bc:bool[] = lt bb 0\n",
       "    bd:i32[] = add bb 10\n",
       "    be:i32[] = select_n bc bb bd\n",
       "    bf:f32[1] = dynamic_slice[slice_sizes=(1,)] a be\n",
       "    bg:f32[] = squeeze[dimensions=(0,)] bf\n",
       "    bh:f32[] = add z bg\n",
       "    bi:i32[1] = slice[limit_indices=(5,) start_indices=(4,) strides=(1,)] b\n",
       "    bj:i32[] = squeeze[dimensions=(0,)] bi\n",
       "    bk:bool[] = lt bj 0\n",
       "    bl:i32[] = add bj 10\n",
       "    bm:i32[] = select_n bk bj bl\n",
       "    bn:f32[1] = dynamic_slice[slice_sizes=(1,)] a bm\n",
       "    bo:f32[] = squeeze[dimensions=(0,)] bn\n",
       "    bp:f32[] = add bh bo\n",
       "    bq:i32[1] = slice[limit_indices=(6,) start_indices=(5,) strides=(1,)] b\n",
       "    br:i32[] = squeeze[dimensions=(0,)] bq\n",
       "    bs:bool[] = lt br 0\n",
       "    bt:i32[] = add br 10\n",
       "    bu:i32[] = select_n bs br bt\n",
       "    bv:f32[1] = dynamic_slice[slice_sizes=(1,)] a bu\n",
       "    bw:f32[] = squeeze[dimensions=(0,)] bv\n",
       "    bx:f32[] = add bp bw\n",
       "    by:i32[1] = slice[limit_indices=(7,) start_indices=(6,) strides=(1,)] b\n",
       "    bz:i32[] = squeeze[dimensions=(0,)] by\n",
       "    ca:bool[] = lt bz 0\n",
       "    cb:i32[] = add bz 10\n",
       "    cc:i32[] = select_n ca bz cb\n",
       "    cd:f32[1] = dynamic_slice[slice_sizes=(1,)] a cc\n",
       "    ce:f32[] = squeeze[dimensions=(0,)] cd\n",
       "    cf:f32[] = add bx ce\n",
       "    cg:i32[1] = slice[limit_indices=(8,) start_indices=(7,) strides=(1,)] b\n",
       "    ch:i32[] = squeeze[dimensions=(0,)] cg\n",
       "    ci:bool[] = lt ch 0\n",
       "    cj:i32[] = add ch 10\n",
       "    ck:i32[] = select_n ci ch cj\n",
       "    cl:f32[1] = dynamic_slice[slice_sizes=(1,)] a ck\n",
       "    cm:f32[] = squeeze[dimensions=(0,)] cl\n",
       "    cn:f32[] = add cf cm\n",
       "    co:i32[1] = slice[limit_indices=(9,) start_indices=(8,) strides=(1,)] b\n",
       "    cp:i32[] = squeeze[dimensions=(0,)] co\n",
       "    cq:bool[] = lt cp 0\n",
       "    cr:i32[] = add cp 10\n",
       "    cs:i32[] = select_n cq cp cr\n",
       "    ct:f32[1] = dynamic_slice[slice_sizes=(1,)] a cs\n",
       "    cu:f32[] = squeeze[dimensions=(0,)] ct\n",
       "    cv:f32[] = add cn cu\n",
       "    cw:i32[1] = slice[limit_indices=(10,) start_indices=(9,) strides=(1,)] b\n",
       "    cx:i32[] = squeeze[dimensions=(0,)] cw\n",
       "    cy:bool[] = lt cx 0\n",
       "    cz:i32[] = add cx 10\n",
       "    da:i32[] = select_n cy cx cz\n",
       "    db:f32[1] = dynamic_slice[slice_sizes=(1,)] a da\n",
       "    dc:f32[] = squeeze[dimensions=(0,)] db\n",
       "    dd:f32[] = add cv dc\n",
       "  in (dd,) }"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:26:04.107717Z",
     "start_time": "2024-11-27T17:26:04.098522Z"
    }
   },
   "source": [
    "make_jaxpr(naive_fori_loop_sum)(jnp.asarray(arr))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[10]. let\n",
       "    _:i32[] b:f32[] = scan[\n",
       "      _split_transpose=False\n",
       "      jaxpr={ lambda ; c:f32[10] d:i32[] e:f32[]. let\n",
       "          f:i32[] = add d 1\n",
       "          g:bool[] = lt d 0\n",
       "          h:i32[] = convert_element_type[new_dtype=int32 weak_type=False] d\n",
       "          i:i32[] = add h 10\n",
       "          j:i32[] = select_n g d i\n",
       "          k:f32[1] = dynamic_slice[slice_sizes=(1,)] c j\n",
       "          l:f32[] = squeeze[dimensions=(0,)] k\n",
       "          m:f32[] = convert_element_type[new_dtype=float32 weak_type=False] e\n",
       "          n:f32[] = add m l\n",
       "        in (f, n) }\n",
       "      length=10\n",
       "      linear=(False, False, False)\n",
       "      num_carry=2\n",
       "      num_consts=1\n",
       "      reverse=False\n",
       "      unroll=1\n",
       "    ] a 0 0.0\n",
       "  in (b,) }"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically, the difference is the number of lines of code generated! It matters for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if we actually wanted to return the cumulative sum array?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:26:04.189418Z",
     "start_time": "2024-11-27T17:26:04.186797Z"
    }
   },
   "source": [
    "def naive_cumsum(xs):\n",
    "    n = xs.shape[0]\n",
    "    res = np.zeros_like(xs)\n",
    "    val = 0.0\n",
    "    for i in range(n):\n",
    "        val = val + xs[i]\n",
    "        res[i] = val\n",
    "    return res\n",
    "\n",
    "\n",
    "print(naive_cumsum(arr))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.  6. 10. 15. 21. 28. 36. 45.]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be equivalent to the following syntax"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:26:04.519083Z",
     "start_time": "2024-11-27T17:26:04.248651Z"
    }
   },
   "source": [
    "def naive_fori_loop_cumsum(xs):\n",
    "    xs = jnp.asarray(xs)\n",
    "    n = xs.shape[0]\n",
    "    res = jnp.zeros_like(xs)\n",
    "\n",
    "    def body(i, val):\n",
    "        # i is the iteration step\n",
    "        # val is the running value\n",
    "        val = val + xs[i]\n",
    "        res[i] = val\n",
    "        return val\n",
    "\n",
    "    _ = fori_loop(\n",
    "        0,  # starting index\n",
    "        n,  # total number of iterations: the last index is n-1\n",
    "        body,  # the function iterated during the loop: res = body(n-1, body(n-2, body(n-3,...)))\n",
    "        init_val=0.0,  # the initial value for the loop\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "print(naive_fori_loop_cumsum(arr))"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "JAX arrays are immutable and do not support in-place item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 19\u001B[0m\n\u001B[1;32m     12\u001B[0m     _ \u001B[38;5;241m=\u001B[39m fori_loop(\u001B[38;5;241m0\u001B[39m,  \u001B[38;5;66;03m# starting index\u001B[39;00m\n\u001B[1;32m     13\u001B[0m                   n,  \u001B[38;5;66;03m# total number of iterations: the last index is n-1\u001B[39;00m\n\u001B[1;32m     14\u001B[0m                   body,  \u001B[38;5;66;03m# the function iterated during the loop: res = body(n-1, body(n-2, body(n-3,...)))\u001B[39;00m\n\u001B[1;32m     15\u001B[0m                   init_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.\u001B[39m  \u001B[38;5;66;03m# the initial value for the loop\u001B[39;00m\n\u001B[1;32m     16\u001B[0m                  )\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mnaive_fori_loop_cumsum\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[0;32mIn[9], line 12\u001B[0m, in \u001B[0;36mnaive_fori_loop_cumsum\u001B[0;34m(xs)\u001B[0m\n\u001B[1;32m      9\u001B[0m     res[i] \u001B[38;5;241m=\u001B[39m val\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m val\n\u001B[0;32m---> 12\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mfori_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# starting index\u001B[39;49;00m\n\u001B[1;32m     13\u001B[0m \u001B[43m              \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# total number of iterations: the last index is n-1\u001B[39;49;00m\n\u001B[1;32m     14\u001B[0m \u001B[43m              \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# the function iterated during the loop: res = body(n-1, body(n-2, body(n-3,...)))\u001B[39;49;00m\n\u001B[1;32m     15\u001B[0m \u001B[43m              \u001B[49m\u001B[43minit_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# the initial value for the loop\u001B[39;49;00m\n\u001B[1;32m     16\u001B[0m \u001B[43m             \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "    \u001B[0;31m[... skipping hidden 12 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[9], line 9\u001B[0m, in \u001B[0;36mnaive_fori_loop_cumsum.<locals>.body\u001B[0;34m(i, val)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbody\u001B[39m(i, val):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# i is the iteration step\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# val is the running value\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     val \u001B[38;5;241m=\u001B[39m val \u001B[38;5;241m+\u001B[39m xs[i]\n\u001B[0;32m----> 9\u001B[0m     \u001B[43mres\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m val\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m val\n",
      "File \u001B[0;32m~/PycharmProjects/.virtualenvs/jax-workshop/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:586\u001B[0m, in \u001B[0;36m_unimplemented_setitem\u001B[0;34m(self, i, x)\u001B[0m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_unimplemented_setitem\u001B[39m(\u001B[38;5;28mself\u001B[39m, i, x):\n\u001B[1;32m    583\u001B[0m   msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJAX arrays are immutable and do not support in-place item assignment.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    584\u001B[0m          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    585\u001B[0m          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 586\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)))\n",
      "\u001B[0;31mTypeError\u001B[0m: JAX arrays are immutable and do not support in-place item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah can't do that actually. The main problem is the line\n",
    "```python\n",
    "res[i] = val\n",
    "```\n",
    "\n",
    "To be able to trace gradients, JAX needs to use pure functions, that is non-inplace updating functions. So to be able to use a JAX specific function that tells it we wish to update the arr `res` at index `i` with the value `val`. This is done using the syntax:\n",
    "```python\n",
    "res = res.at[i].set(val)\n",
    "```\n",
    "While it may seem like a copy of the array is made at each iteration, JAX is smart enough to only update the necessary values. Let's see how this looks like in code:  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:27:41.390767Z",
     "start_time": "2024-11-27T17:27:41.357080Z"
    }
   },
   "source": [
    "def not_so_naive_fori_loop_cumsum(xs):\n",
    "    xs = jnp.asarray(xs)\n",
    "    n = xs.shape[0]\n",
    "\n",
    "    def body(i, carry):\n",
    "        # i is the iteration step\n",
    "        # carry is the running value\n",
    "        cumsum, val = carry\n",
    "        val = val + xs[i]\n",
    "        cumsum = cumsum.at[i].set(val)\n",
    "        return cumsum, val\n",
    "\n",
    "    res, val = fori_loop(0, n, body, init_val=(jnp.zeros_like(xs), 0.0))\n",
    "    return res\n",
    "\n",
    "\n",
    "print(not_so_naive_fori_loop_cumsum(arr))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.  6. 10. 15. 21. 28. 36. 45.]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This is starting to become awfully complicated just for a simple cumulative sum, thankfully, JAX has a `scan` operator that solves just this problem and accumulates the values for you:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:27:16.887057Z",
     "start_time": "2024-11-27T17:27:16.860242Z"
    }
   },
   "source": [
    "def naive_scan_cumsum(xs):\n",
    "    def body(val, x):\n",
    "        val = val + x\n",
    "        return (\n",
    "            val,\n",
    "            val,\n",
    "        )  # the first output is the carry, the second one is the recorded value at the current index\n",
    "\n",
    "    _final_val, cumsum = scan(\n",
    "        body,  # function with signature (carry, input)-> (new_carry, res[i])\n",
    "        0.0,  # the initial value\n",
    "        xs,  # the list of inputs x to the body function\n",
    "    )\n",
    "    return cumsum  # in this example we just don't need the final value\n",
    "\n",
    "\n",
    "print(naive_scan_cumsum(arr))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.  6. 10. 15. 21. 28. 36. 45.]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally imagine we want to compute the sum up to a threshold:\n",
    "```python\n",
    "def sum_to_threshold(xs, thresh):\n",
    "    res = 0.\n",
    "    i = 0\n",
    "    while True:\n",
    "        new_res = xs[i] + res\n",
    "        if new_res > thresh:\n",
    "            break\n",
    "        res = new_res\n",
    "        i += 1\n",
    "    return res\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we would use a while loop:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T17:28:16.771962Z",
     "start_time": "2024-11-27T17:28:16.750780Z"
    }
   },
   "source": [
    "def while_loop_sum_to_threshold(xs, thresh):\n",
    "    xs = jnp.asarray(xs)\n",
    "    n = xs.shape[0]\n",
    "\n",
    "    def cond(carry):\n",
    "        i, val = carry\n",
    "        return jnp.logical_and((i < n), (val + xs[i] < thresh))  # if true we continue\n",
    "\n",
    "    def body(carry):\n",
    "        i, val = carry\n",
    "        return i + 1, val + xs[i]\n",
    "\n",
    "    _, val = while_loop(cond, body, init_val=(0, 0.0))\n",
    "    return val\n",
    "\n",
    "\n",
    "print(while_loop_sum_to_threshold(arr, 11.0))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: \n",
    "Using `scan` with a fixed number of iterations implement the Newton square root algorithm which in pure python is given by:\n",
    "```python\n",
    "def sqrt(x, x0, N):\n",
    "    y = x0\n",
    "    for _ in range(N):\n",
    "        y = 0.5 * (y + x / y)\n",
    "    return y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "Using `while_loop` implement the searchsorted function:\n",
    "```python\n",
    "def searchsorted(x, arr):\n",
    "    i = 0\n",
    "    while i < len(arr):\n",
    "        if arr[i] >= x:\n",
    "            return i\n",
    "        i += 1\n",
    "    return i\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate\n",
    "### Prerequisites\n",
    "- Beginner loops\n",
    "- Beginner automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import make_jaxpr, grad, jvp\n",
    "from jax.lax import while_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to compute loops, how does it bode in terms of gradients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the respective behaviour state of `while_loop` versus `scan` operations (the `fori_loop` is in a stage of limbo at the time of this workshop writing but should eventually be implemented as a `scan` operation) we will consider the newton square root toy example from the Beginners question Q1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70710677\n"
     ]
    }
   ],
   "source": [
    "def while_loop_sqrt(x, x0=1.0, n_iter=10):\n",
    "    def cond(carry):\n",
    "        i, _val = carry\n",
    "        return i < n_iter\n",
    "\n",
    "    def body(carry):\n",
    "        i, val = carry\n",
    "        return i + 1, 0.5 * (val + x / val)\n",
    "\n",
    "    _, res = while_loop(cond, body, (0, x0))\n",
    "    return res\n",
    "\n",
    "\n",
    "print(while_loop_sqrt(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70710677\n"
     ]
    }
   ],
   "source": [
    "def scan_loop_sqrt(x, x0=1.0, n_iter=10):\n",
    "    def body(val, _):\n",
    "        return 0.5 * (val + x / val), None\n",
    "\n",
    "    res, _ = scan(body, x0, jnp.arange(n_iter))\n",
    "    return res\n",
    "\n",
    "\n",
    "print(scan_loop_sqrt(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the main difference is that the scan version number of iterations is known in advance, whereas the while_loop is a bit more flexible so as to what the stopping condition is (here I put the same one to compare, but one may imagine using a precision threshold instead). This has a very important implication in terms of gradients (theoretically in terms of performance too on GPU, but as we speak the implementation is not full GPU supported).\n",
    "\n",
    "Let's first look at the code generated by the `while_loop` and the `scan` respective implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let _ b = while[ body_jaxpr={ lambda  ; a b c.\n",
       "                                let d = add b 1\n",
       "                                    e = div a c\n",
       "                                    f = add c e\n",
       "                                    g = mul f 0.5\n",
       "                                in (d, g) }\n",
       "                   body_nconsts=1\n",
       "                   cond_jaxpr={ lambda  ; a b.\n",
       "                                let c = lt a 10\n",
       "                                in (c,) }\n",
       "                   cond_nconsts=0 ] a 0 1.0\n",
       "  in (b,) }"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(while_loop_sqrt)(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let b = iota[ dimension=0\n",
       "                dtype=int32\n",
       "                shape=(10,) ] \n",
       "      c = scan[ jaxpr={ lambda  ; a b c.\n",
       "                        let d = div a b\n",
       "                            e = add b d\n",
       "                            f = mul e 0.5\n",
       "                        in (f,) }\n",
       "                length=10\n",
       "                linear=(False, False, False)\n",
       "                num_carry=1\n",
       "                num_consts=1\n",
       "                reverse=False\n",
       "                unroll=1 ] a 1.0 b\n",
       "  in (c,) }"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(scan_loop_sqrt)(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see how this translates in terms of gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let b = iota[ dimension=0\n",
       "                dtype=int32\n",
       "                shape=(10,) ] \n",
       "      _ _ c _ d =\n",
       "        scan[ jaxpr={ lambda  ; f a b c d e.\n",
       "                      let g = div f c\n",
       "                          h = add c g\n",
       "                          i = mul h 0.5\n",
       "                          j = integer_pow[ y=-2 ] c\n",
       "                      in (i, *, c, *, j) }\n",
       "              length=10\n",
       "              linear=(False, True, True, False, True, False)\n",
       "              num_carry=2\n",
       "              num_consts=3\n",
       "              reverse=False\n",
       "              unroll=1 ] a * * 1.0 * b\n",
       "      _ e _ _ _ =\n",
       "        scan[ jaxpr={ lambda  ; a b c d e f g.\n",
       "                      let h = mul e 0.5\n",
       "                          i = mul h g\n",
       "                          j = mul i a\n",
       "                          k = neg j\n",
       "                          l = add_any h k\n",
       "                          m = div h f\n",
       "                          n = add_any c m\n",
       "                      in (b, n, *, l, *) }\n",
       "              length=10\n",
       "              linear=(False, True, True, True, True, False, False)\n",
       "              num_carry=4\n",
       "              num_consts=1\n",
       "              reverse=True\n",
       "              unroll=1 ] a * 0.0 * 1.0 c d\n",
       "  in (e,) }"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(grad(scan_loop_sqrt))(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.5, dtype=float32), DeviceArray(1., dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jvp(scan_loop_sqrt, (0.25,), (1.0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.5, dtype=float32), DeviceArray(1., dtype=float32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jvp(while_loop_sqrt, (0.25,), (1.0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(1., dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(scan_loop_sqrt)(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFilteredStackTrace\u001B[0m                        Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-b45ec8a825bd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mgrad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwhile_loop_sqrt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.25\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/_src/lax/control_flow.py\u001B[0m in \u001B[0;36m_while_transpose_error\u001B[0;34m(*_, **kwargs)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_while_transpose_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 538\u001B[0;31m   raise ValueError(\"Reverse-mode differentiation does not work for \"\n\u001B[0m\u001B[1;32m    539\u001B[0m                    \u001B[0;34m\"lax.while_loop or lax.fori_loop. \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFilteredStackTrace\u001B[0m: ValueError: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead.\n\nThe stack trace above excludes JAX-internal frames.\nThe following is the original exception that occurred, unmodified.\n\n--------------------",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-b45ec8a825bd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mgrad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwhile_loop_sqrt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.25\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/traceback_util.py\u001B[0m in \u001B[0;36mreraise_with_filtered_traceback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mreraise_with_filtered_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 133\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    134\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_under_reraiser\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/api.py\u001B[0m in \u001B[0;36mgrad_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    750\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mapi_boundary\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    751\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mgrad_f\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 752\u001B[0;31m     \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue_and_grad_f\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    753\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mg\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    754\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/traceback_util.py\u001B[0m in \u001B[0;36mreraise_with_filtered_traceback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mreraise_with_filtered_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 133\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    134\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_under_reraiser\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/api.py\u001B[0m in \u001B[0;36mvalue_and_grad_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    819\u001B[0m     \u001B[0mdtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresult_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mans\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    820\u001B[0m     \u001B[0mtree_map\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartial\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_check_output_dtype_grad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mholomorphic\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mans\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 821\u001B[0;31m     \u001B[0mg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvjp_py\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mones\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    822\u001B[0m     \u001B[0mg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mg\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margnums\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mg\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    823\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mhas_aux\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/api.py\u001B[0m in \u001B[0;36m_vjp_pullback_wrapper\u001B[0;34m(cotangent_dtypes, io_tree, fun, py_args)\u001B[0m\n\u001B[1;32m   1791\u001B[0m              \"with dtype {}.\")\n\u001B[1;32m   1792\u001B[0m       \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mct_dtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexpected_tangent_dtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_dtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1793\u001B[0;31m   \u001B[0mans\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1794\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mtree_unflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout_tree\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mans\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1795\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/interpreters/ad.py\u001B[0m in \u001B[0;36munbound_vjp\u001B[0;34m(pvals, jaxpr, consts, *cts)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[0mcts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mignore_consts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m     \u001B[0mdummy_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mUndefinedPrimal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maval\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mjaxpr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minvars\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m     \u001B[0marg_cts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbackward_pass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjaxpr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconsts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdummy_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstantiate_zeros\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg_cts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/interpreters/ad.py\u001B[0m in \u001B[0;36mbackward_pass\u001B[0;34m(jaxpr, consts, primals_in, cotangents_in)\u001B[0m\n\u001B[1;32m    219\u001B[0m       \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m         cts_out = get_primitive_transpose(eqn.primitive)(cts_in, *invals,\n\u001B[0;32m--> 221\u001B[0;31m                                                          **eqn.params)\n\u001B[0m\u001B[1;32m    222\u001B[0m     \u001B[0mcts_out\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mZero\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maval\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0meqn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minvars\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mcts_out\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mZero\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mcts_out\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[0;31m# FIXME: Some invars correspond to primals!\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/_src/lax/control_flow.py\u001B[0m in \u001B[0;36m_while_transpose_error\u001B[0;34m(*_, **kwargs)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_while_transpose_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 538\u001B[0;31m   raise ValueError(\"Reverse-mode differentiation does not work for \"\n\u001B[0m\u001B[1;32m    539\u001B[0m                    \u001B[0;34m\"lax.while_loop or lax.fori_loop. \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    540\u001B[0m                    \"Try using lax.scan instead.\")\n",
      "\u001B[0;31mValueError\u001B[0m: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead."
     ]
    }
   ],
   "source": [
    "grad(while_loop_sqrt)(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the automatic differentiation, reverse-mode needs to allocate memory, which can't be done dynamically in XLA, so that JAX doesn't allow for reverse mode differentiation through while_loops which could grow indefinitely. Instead some work has been planned in JAX to allow for bounded size while loops to be implemented using the scan syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: \n",
    "Implement the following bubble sort algorithm:\n",
    "```python\n",
    "def bubble_sort(arr): \n",
    "    n = len(arr) \n",
    "    res = np.copy(arr)\n",
    "    for i in range(n-1): \n",
    "        for j in range(0, n-i-1): \n",
    "            if res[j] > res[j+1]: \n",
    "                res[j], res[j+1] = res[j+1], res[j]\n",
    "    return res   \n",
    "```\n",
    "What is its Jacobian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "Implement the following discrete [Hidden Markov Model](https://en.wikipedia.org/wiki/Hidden_Markov_model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_filter(A, B, pi, ys):\n",
    "    pxs = np.empty((ys.shape[0], pi.shape[0]))\n",
    "    for i, y in enumerate(ys):\n",
    "        B_y = B[:, y]  # likelihood\n",
    "        px = B_y * px  # unormalised bayes rule\n",
    "        px = px / px.sum()  # normalisation\n",
    "        pxs[i] = px  # registration\n",
    "        px = A @ px  # prediction\n",
    "    return pxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "### Prerequisites\n",
    "- Intermediate loops "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import make_jaxpr, jvp, vjp\n",
    "from jax.lax import associative_scan, scan\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now present an additional primitive which can be useful when dealing with associative binary operations (such as summation): `associative_scan`, also known as [prefix sum](https://en.wikipedia.org/wiki/Prefix_sum). It consists in applying recursive operations to subsets of the inputs (divide and conquer strategy) instead of applying it sequentially. This has the benefit of being easily parallelisable and is natively implemented in JAX. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it implements a parallelised version (see for example the [Wikipedia](https://en.wikipedia.org/wiki/Prefix_sum)) of the following algorithm:\n",
    "```python\n",
    "def my_sequential_associative_scan(binary_op, xs):\n",
    "    res = np.copy(xs)\n",
    "    val = xs[0]\n",
    "    for i, x in enumerate(xs[1:]):\n",
    "        val = binary_op(val, x)\n",
    "        res[i+1] = val\n",
    "    return res\n",
    "```\n",
    "\n",
    "so that the cumulative sum would be for example written as \n",
    "```python\n",
    "my_sequential_associative_scan(lambda x, y: x + y, np.arange(10))\n",
    "```\n",
    "In pure python, this would typically look like:\n",
    "```python\n",
    "def home_made_blelloch(arr):\n",
    "    # This is for illustration purposes only, and for instance doesn't take into\n",
    "    # account the case when the array size is not a pure power of 2\n",
    "    res = np.copy(arr)\n",
    "    n = res.shape[0]\n",
    "    log_n = np.log2(n).astype(int)\n",
    "\n",
    "    # Up pass\n",
    "    for d in range(log_n):\n",
    "        # this loop can't be done in parallel so it defines the span complexity under\n",
    "        # parallelization\n",
    "        for i in range(0, n, 2 ** (d + 1)):\n",
    "            # this should be done in parallel, therefore would not be taken\n",
    "            # into account in the span complexity provided we have at least\n",
    "            # n / 2^{d+1} cores on our GPU\n",
    "            i1 = i + 2 ** d - 1\n",
    "            i2 = i + 2 ** (d + 1) - 1\n",
    "            res[i2] += res[i1]\n",
    "\n",
    "    res[-1] = 0\n",
    "\n",
    "    # Down pass\n",
    "    for d in range(log_n - 1, -1, -1):\n",
    "        # this loop can't be done in parallel so it defines the span complexity under\n",
    "        # parallelization\n",
    "        for i in range(0, n, 2 ** (d + 1)):\n",
    "            # this should be done in parallel, therefore would not be taken\n",
    "            # into account in the span complexity provided we have at least\n",
    "            # n / 2^{d+1} cores on our GPU\n",
    "            i1 = i + 2 ** d - 1\n",
    "            i2 = i + 2 ** (d + 1) - 1\n",
    "\n",
    "            res[i1], res[i2] = res[i2], res[i1] + res[i2]\n",
    "            # Extra pass\n",
    "    res += arr\n",
    "\n",
    "    return res\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using JAX this would actually be written in the following wa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.  6. 10. 15. 21. 28. 36. 45.]\n"
     ]
    }
   ],
   "source": [
    "def associative_cumulative_sum(xs):\n",
    "    return associative_scan(lambda x, y: x + y, xs)\n",
    "\n",
    "\n",
    "print(associative_cumulative_sum(np.arange(10.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the code generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let b = slice[ limit_indices=(9,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(2,) ] a\n",
       "      c = slice[ limit_indices=(10,)\n",
       "                 start_indices=(1,)\n",
       "                 strides=(2,) ] a\n",
       "      d = add b c\n",
       "      e = slice[ limit_indices=(4,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(2,) ] d\n",
       "      f = slice[ limit_indices=(5,)\n",
       "                 start_indices=(1,)\n",
       "                 strides=(2,) ] d\n",
       "      g = add e f\n",
       "      h = slice[ limit_indices=(1,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(2,) ] g\n",
       "      i = slice[ limit_indices=(2,)\n",
       "                 start_indices=(1,)\n",
       "                 strides=(2,) ] g\n",
       "      j = add h i\n",
       "      k = slice[ limit_indices=(0,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] j\n",
       "      l = slice[ limit_indices=(2,)\n",
       "                 start_indices=(2,)\n",
       "                 strides=(2,) ] g\n",
       "      m = add k l\n",
       "      n = slice[ limit_indices=(1,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] g\n",
       "      o = concatenate[ dimension=0 ] n m\n",
       "      p = pad[ padding_config=((0, 1, 1),) ] o 0.0\n",
       "      q = pad[ padding_config=((1, 0, 1),) ] j 0.0\n",
       "      r = add p q\n",
       "      s = slice[ limit_indices=(5,)\n",
       "                 start_indices=(2,)\n",
       "                 strides=(2,) ] d\n",
       "      t = add r s\n",
       "      u = slice[ limit_indices=(1,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] d\n",
       "      v = concatenate[ dimension=0 ] u t\n",
       "      w = pad[ padding_config=((0, 0, 1),) ] v 0.0\n",
       "      x = pad[ padding_config=((1, 1, 1),) ] r 0.0\n",
       "      y = add w x\n",
       "      z = slice[ limit_indices=(4,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] y\n",
       "      ba = slice[ limit_indices=(10,)\n",
       "                  start_indices=(2,)\n",
       "                  strides=(2,) ] a\n",
       "      bb = add z ba\n",
       "      bc = slice[ limit_indices=(1,)\n",
       "                  start_indices=(0,)\n",
       "                  strides=(1,) ] a\n",
       "      bd = concatenate[ dimension=0 ] bc bb\n",
       "      be = pad[ padding_config=((0, 1, 1),) ] bd 0.0\n",
       "      bf = pad[ padding_config=((1, 0, 1),) ] y 0.0\n",
       "      bg = add be bf\n",
       "  in (bg,) }"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(associative_cumulative_sum)(np.arange(10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks just as when we were doing python loops! Is it bad? No, because the depth of the generated graph will only grow in $\\log_2(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as for `scan`, it is closed under differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DeviceArray([ 0.,  1.,  3.,  6., 10.], dtype=float32), DeviceArray([1., 2., 3., 4., 5.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(jvp(associative_cumulative_sum, (np.arange(5.0),), (np.ones(5),)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.  6. 10.]\n",
      "(DeviceArray([5., 4., 3., 2., 1.], dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "val, cumsum_bwd = vjp(associative_cumulative_sum, np.arange(5.0))\n",
    "print(val)\n",
    "print(cumsum_bwd(np.ones(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: \n",
    "Compare the speed of the associative_scan and scan implementation of cumulative sum on GPU and CPU (use the device flag in the jit function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "A simpler version of parallel scans is given by Hillis and Steele's algorithm, which is given in a pure python version by:\n",
    "```python\n",
    "def home_made_hillis_and_steel(arr):\n",
    "    # This is for illustration purposes only\n",
    "    res = np.copy(arr)\n",
    "    n = res.shape[0]\n",
    "    log_n = np.log2(n).astype(int)\n",
    "    n_operations = 0\n",
    "    for d in range(log_n):\n",
    "        # this loop can't be done in parallel so it defines the span complexity under\n",
    "        # parallelization\n",
    "        for i in reversed(range(n)):\n",
    "            # For each i, in parallel\n",
    "            if i - 2 ** d >= 0:\n",
    "                n_operations += 1\n",
    "                res[i] += res[i - 2 ** d]\n",
    "    return res, n_operations\n",
    "```\n",
    "Implements your own parallel version of `associative_scan` using jax primitives, using the Hillis and Steele's algorithm, or Blelloch's algorithm if you feel adventurous."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
