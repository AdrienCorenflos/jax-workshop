{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn the different ways to implement loops (with fixed size or dynamic size) using JAX primitives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginner\n",
    "### Prerequisites\n",
    "No prerequisite - (Beginner if-else is better though)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import make_jaxpr\n",
    "from jax.lax import while_loop, fori_loop, scan\n",
    "import jax.numpy as jnp\n",
    "from jax.ops import index_update\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first give an example of how to compute the cumulative sum of the values in an array of floats using JAX:\n",
    "```python\n",
    "def my_cumsum(xs):\n",
    "    res = np.zeros_like(xs)\n",
    "    for i, x in enumerate(xs):\n",
    "        res[i] = x + res[i-1]\n",
    "    return res\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following array for our tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(0., 10.)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FORI_LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most natural syntax would seem to be to use the fori_loop: let's first have a look at what simply summing the array would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n"
     ]
    }
   ],
   "source": [
    "def naive_sum(xs):\n",
    "    n = xs.shape[0]\n",
    "    val = 0.\n",
    "    for i in jnp.arange(n):\n",
    "        val = val + xs[i]\n",
    "    return val\n",
    "\n",
    "print(naive_sum(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use this directly on a jax array then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(45., dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_sum(jnp.asarray(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well yes! So why all the fuss? Surely we can just not care right? Let write down a (bad) equivalent in jax and check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n"
     ]
    }
   ],
   "source": [
    "def naive_fori_loop_sum(xs):\n",
    "    xs = jnp.asarray(xs) \n",
    "    n = xs.shape[0]\n",
    "    def body(i, val):\n",
    "        # i is the iteration step\n",
    "        # val is the running value\n",
    "        val = val + xs[i]\n",
    "        return val\n",
    "    \n",
    "    res = fori_loop(0,  # starting index\n",
    "                  n,  # total number of iterations: the last index is n-1\n",
    "                  body,  # the function iterated during the loop: res = body(n-1, body(n-2, body(n-3,...)))\n",
    "                  init_val=0.  # the initial value for the loop\n",
    "                 )\n",
    "    return res\n",
    "\n",
    "print(naive_fori_loop_sum(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda a b c d e f g h i j ; k.\n",
       "  let l = iota[ dimension=0\n",
       "                dtype=int32\n",
       "                shape=(10,) ] \n",
       "      m = slice[ limit_indices=(1,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] l\n",
       "      n = squeeze[ dimensions=(0,) ] m\n",
       "      o = slice[ limit_indices=(2,)\n",
       "                 start_indices=(1,)\n",
       "                 strides=(1,) ] l\n",
       "      p = squeeze[ dimensions=(0,) ] o\n",
       "      q = slice[ limit_indices=(3,)\n",
       "                 start_indices=(2,)\n",
       "                 strides=(1,) ] l\n",
       "      r = squeeze[ dimensions=(0,) ] q\n",
       "      s = slice[ limit_indices=(4,)\n",
       "                 start_indices=(3,)\n",
       "                 strides=(1,) ] l\n",
       "      t = squeeze[ dimensions=(0,) ] s\n",
       "      u = slice[ limit_indices=(5,)\n",
       "                 start_indices=(4,)\n",
       "                 strides=(1,) ] l\n",
       "      v = squeeze[ dimensions=(0,) ] u\n",
       "      w = slice[ limit_indices=(6,)\n",
       "                 start_indices=(5,)\n",
       "                 strides=(1,) ] l\n",
       "      x = squeeze[ dimensions=(0,) ] w\n",
       "      y = slice[ limit_indices=(7,)\n",
       "                 start_indices=(6,)\n",
       "                 strides=(1,) ] l\n",
       "      z = squeeze[ dimensions=(0,) ] y\n",
       "      ba = slice[ limit_indices=(8,)\n",
       "                  start_indices=(7,)\n",
       "                  strides=(1,) ] l\n",
       "      bb = squeeze[ dimensions=(0,) ] ba\n",
       "      bc = slice[ limit_indices=(9,)\n",
       "                  start_indices=(8,)\n",
       "                  strides=(1,) ] l\n",
       "      bd = squeeze[ dimensions=(0,) ] bc\n",
       "      be = slice[ limit_indices=(10,)\n",
       "                  start_indices=(9,)\n",
       "                  strides=(1,) ] l\n",
       "      bf = squeeze[ dimensions=(0,) ] be\n",
       "      bg = lt n 0\n",
       "      bh = add n 10\n",
       "      bi = select bg bh n\n",
       "      bj = broadcast_in_dim[ broadcast_dimensions=(  )\n",
       "                             shape=(1,) ] bi\n",
       "      bk = concatenate[ dimension=0 ] a bj\n",
       "      bl = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))\n",
       "                   slice_sizes=(1,) ] k bk\n",
       "      bm = add bl 0.0\n",
       "      bn = lt p 0\n",
       "      bo = add p 10\n",
       "      bp = select bn bo p\n",
       "      bq = broadcast_in_dim[ broadcast_dimensions=(  )\n",
       "                             shape=(1,) ] bp\n",
       "      br = concatenate[ dimension=0 ] b bq\n",
       "      bs = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))\n",
       "                   slice_sizes=(1,) ] k br\n",
       "      bt = add bm bs\n",
       "      bu = lt r 0\n",
       "      bv = add r 10\n",
       "      bw = select bu bv r\n",
       "      bx = broadcast_in_dim[ broadcast_dimensions=(  )\n",
       "                             shape=(1,) ] bw\n",
       "      by = concatenate[ dimension=0 ] c bx\n",
       "      bz = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))\n",
       "                   slice_sizes=(1,) ] k by\n",
       "      ca = add bt bz\n",
       "      cb = lt t 0\n",
       "      cc = add t 10\n",
       "      cd = select cb cc t\n",
       "      ce = broadcast_in_dim[ broadcast_dimensions=(  )\n",
       "                             shape=(1,) ] cd\n",
       "      cf = concatenate[ dimension=0 ] d ce\n",
       "      cg = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))\n",
       "                   slice_sizes=(1,) ] k cf\n",
       "      ch = add ca cg\n",
       "      ci = lt v 0\n",
       "      cj = add v 10\n",
       "      ck = select ci cj v\n",
       "      cl = broadcast_in_dim[ broadcast_dimensions=(  )\n",
       "                             shape=(1,) ] ck\n",
       "      cm = concatenate[ dimension=0 ] e cl\n",
       "      cn = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))\n",
       "                   slice_sizes=(1,) ] k cm\n",
       "      co = add ch cn\n",
       "      cp = lt x 0\n",
       "      cq = add x 10\n",
       "      cr = select cp cq x\n",
       "      cs = broadcast_in_dim[ broadcast_dimensions=(  )\n",
       "                             shape=(1,) ] cr\n",
       "      ct = concatenate[ dimension=0 ] f cs\n",
       "      cu = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))\n",
       "                   slice_sizes=(1,) ] k ct\n",
       "      cv = add co cu\n",
       "      cw = lt z 0\n",
       "      cx = add z 10\n",
       "      cy = select cw cx z\n",
       "      cz = broadcast_in_dim[ broadcast_dimensions=(  )\n",
       "                             shape=(1,) ] cy\n",
       "      da = concatenate[ dimension=0 ] g cz\n",
       "      db = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))\n",
       "                   slice_sizes=(1,) ] k da\n",
       "      dc = add cv db\n",
       "      dd = lt bb 0\n",
       "      de = add bb 10\n",
       "      df = select dd de bb\n",
       "      dg = broadcast_in_dim[ broadcast_dimensions=(  )\n",
       "                             shape=(1,) ] df\n",
       "      dh = concatenate[ dimension=0 ] h dg\n",
       "      di = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))\n",
       "                   slice_sizes=(1,) ] k dh\n",
       "      dj = add dc di\n",
       "      dk = lt bd 0\n",
       "      dl = add bd 10\n",
       "      dm = select dk dl bd\n",
       "      dn = broadcast_in_dim[ broadcast_dimensions=(  )\n",
       "                             shape=(1,) ] dm\n",
       "      do = concatenate[ dimension=0 ] i dn\n",
       "      dp = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))\n",
       "                   slice_sizes=(1,) ] k do\n",
       "      dq = add dj dp\n",
       "      dr = lt bf 0\n",
       "      ds = add bf 10\n",
       "      dt = select dr ds bf\n",
       "      du = broadcast_in_dim[ broadcast_dimensions=(  )\n",
       "                             shape=(1,) ] dt\n",
       "      dv = concatenate[ dimension=0 ] j du\n",
       "      dw = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))\n",
       "                   slice_sizes=(1,) ] k dv\n",
       "      dx = add dq dw\n",
       "  in (dx,) }"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(naive_sum)(jnp.asarray(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda a ; b.\n",
       "  let _ _ c = while[ body_jaxpr={ lambda  ; a b c d e.\n",
       "                                  let f = add c 1\n",
       "                                      g = lt c 0\n",
       "                                      h = add c 10\n",
       "                                      i = select g h c\n",
       "                                      j = broadcast_in_dim[ broadcast_dimensions=(  )\n",
       "                                                            shape=(1,) ] i\n",
       "                                      k = concatenate[ dimension=0 ] a j\n",
       "                                      l = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))\n",
       "                                                  slice_sizes=(1,) ] b k\n",
       "                                      m = add e l\n",
       "                                  in (f, d, m) }\n",
       "                     body_nconsts=2\n",
       "                     cond_jaxpr={ lambda  ; a b c.\n",
       "                                  let d = lt a b\n",
       "                                  in (d,) }\n",
       "                     cond_nconsts=0 ] a b 0 10 0.0\n",
       "  in (c,) }"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(naive_fori_loop_sum)(jnp.asarray(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically, the difference is the number of lines of code generated! It matters for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if we actually wanted to return the cumulative sum array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'naive_numpy_cumsum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-31df9db9a5ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_numpy_cumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'naive_numpy_cumsum' is not defined"
     ]
    }
   ],
   "source": [
    "def naive_cumsum(xs):\n",
    "    n = xs.shape[0]\n",
    "    res = np.zeros_like(xs)\n",
    "    val = 0.\n",
    "    for i in range(n):\n",
    "        val = val + xs[i]\n",
    "        res[i] = val\n",
    "    return res\n",
    "\n",
    "print(naive_numpy_cumsum(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be equivalent to the following syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_fori_loop_cumsum(xs):\n",
    "    xs = jnp.asarray(xs)\n",
    "    n = xs.shape[0]\n",
    "    res = jnp.zeros_like(xs)\n",
    "    def body(i, val):\n",
    "        # i is the iteration step\n",
    "        # val is the running value\n",
    "        val = val + xs[i]\n",
    "        res[i] = val\n",
    "        return val\n",
    "    \n",
    "    _ = fori_loop(0,  # starting index\n",
    "                  n,  # total number of iterations: the last index is n-1\n",
    "                  body,  # the function iterated during the loop: res = body(n-1, body(n-2, body(n-3,...)))\n",
    "                  init_val=0.  # the initial value for the loop\n",
    "                 )\n",
    "    return res\n",
    "\n",
    "print(naive_fori_loop_cumsum(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah can't do that actually. The main problem is the line\n",
    "```python\n",
    "res[i] = val\n",
    "```\n",
    "\n",
    "To be able to trace gradients, JAX needs to use pure functions, that is non-inline functions. So to be able to use the loop syntax we would need to use a JAX specific operation: `index_update`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_so_naive_fori_loop_cumsum(xs):\n",
    "    xs = jnp.asarray(xs)\n",
    "    n = xs.shape[0]\n",
    "    res = jnp.zeros_like(xs)\n",
    "    def body(i, carry):\n",
    "        # i is the iteration step\n",
    "        # carry is the running value\n",
    "        cumsum, val = carry\n",
    "        val = val + xs[i]\n",
    "        cumsum = index_update(cumsum, i, val)\n",
    "        return cumsum, val\n",
    "    \n",
    "    res, val = fori_loop(0, n, body, init_val=(jnp.zeros_like(xs), 0.))\n",
    "    return res\n",
    "\n",
    "print(not_so_naive_fori_loop_cumsum(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to become awfully complicated just for a simple cumulative sum, thankfully, JAX has a `scan` operator that solves just this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_scan_cumsum(xs):\n",
    "    def body(val, x):\n",
    "        val = val + x\n",
    "        return val, val  # the first output is the carry, the second one is the recorded value at the current index\n",
    "    \n",
    "    _final_val, cumsum = scan(body,  # function with signature (carry, input)-> (new_carry, res[i])\n",
    "                              0.,  # the initial value\n",
    "                              xs  # the list of inputs x to the body function\n",
    "                             )\n",
    "    return cumsum  # in this example we just don't need the final value\n",
    "\n",
    "print(naive_scan_cumsum(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally imagine we want to compute the sum up to a threshold:\n",
    "```python\n",
    "def sum_to_threshold(xs, thresh):\n",
    "    res = 0.\n",
    "    i = 0\n",
    "    while True:\n",
    "        new_res = xs[i] + res\n",
    "        if new_res > thresh:\n",
    "            break\n",
    "        res = new_res\n",
    "        i += 1\n",
    "    return res\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we would use a while loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def while_loop_sum_to_threshold(xs, thresh):\n",
    "    xs = jnp.asarray(xs)\n",
    "    n = xs.shape[0]\n",
    "    \n",
    "    def cond(carry):\n",
    "        i, val = carry\n",
    "        return jnp.logical_and((i < n), \n",
    "                               (val + xs[i] < thresh))  # if true we continue\n",
    "    \n",
    "    def body(carry):        \n",
    "        i, val = carry\n",
    "        return i + 1, val + xs[i]\n",
    "    \n",
    "    _, val = while_loop(cond, \n",
    "                        body,\n",
    "                        init_val=(0, 0.)\n",
    "                        )\n",
    "    return val\n",
    "\n",
    "print(while_loop_sum_to_threshold(arr, 11.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: \n",
    "Using `scan` with a fixed number of iterations implement the Newton square root algorithm which in pure python is given by:\n",
    "```python\n",
    "def sqrt(x, x0, N):\n",
    "    y = x0\n",
    "    for _ in range(N):\n",
    "        y = 0.5 * (y + x / y)\n",
    "    return y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "Using `while_loop` implement the searchsorted function:\n",
    "```python\n",
    "def searchsorted(x, arr):\n",
    "    i = 0\n",
    "    while i < len(arr):\n",
    "        if arr[i] >= x:\n",
    "            return i\n",
    "        i += 1\n",
    "    return i\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate\n",
    "### Prerequisites\n",
    "- Beginner loops\n",
    "- Beginner automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import make_jaxpr, device_put, device_get, jit, grad, jvp\n",
    "from jax.lax import scan, while_loop\n",
    "import jax.numpy as jax_np\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to compute loops, how does it bode in terms of gradients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the respective behaviour state of `while_loop` versus `scan` operations (the `fori_loop` is in a stage of limbo at the time of this workshop writing but should eventually be implemented as a `scan` operation) we will consider the newton square root toy example from the Beginners question Q1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70710677\n"
     ]
    }
   ],
   "source": [
    "def while_loop_sqrt(x, x0=1., n_iter=10):\n",
    "    \n",
    "    def cond(carry):\n",
    "        i, _val = carry\n",
    "        return i < n_iter\n",
    "    \n",
    "    def body(carry):\n",
    "        i, val = carry\n",
    "        return i+1, 0.5 * (val + x / val)\n",
    "    \n",
    "    _, res = while_loop(cond, body, (0,  x0))\n",
    "    return res\n",
    "\n",
    "print(while_loop_sqrt(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70710677\n"
     ]
    }
   ],
   "source": [
    "def scan_loop_sqrt(x, x0=1., n_iter=10):\n",
    "    \n",
    "    def body(val, _):\n",
    "        return 0.5 * (val + x / val), None\n",
    "    res, _ = scan(body, x0, jnp.arange(n_iter))\n",
    "    return res\n",
    "\n",
    "print(scan_loop_sqrt(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the main difference is that the scan version number of iterations is known in advance, whereas the while_loop is a bit more flexible so as to what the stopping condition is (here I put the same one to compare, but one may imagine using a precision threshold instead). This has a very important implication in terms of gradients (theoretically in terms of performance too on GPU, but as we speak the implementation is not full GPU supported).\n",
    "\n",
    "Let's first look at the code generated by the `while_loop` and the `scan` respective implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let _ b = while[ body_jaxpr={ lambda  ; a b c.\n",
       "                                let d = add b 1\n",
       "                                    e = div a c\n",
       "                                    f = add c e\n",
       "                                    g = mul f 0.5\n",
       "                                in (d, g) }\n",
       "                   body_nconsts=1\n",
       "                   cond_jaxpr={ lambda  ; a b.\n",
       "                                let c = lt a 10\n",
       "                                in (c,) }\n",
       "                   cond_nconsts=0 ] a 0 1.0\n",
       "  in (b,) }"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(while_loop_sqrt)(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let b = iota[ dimension=0\n",
       "                dtype=int32\n",
       "                shape=(10,) ] \n",
       "      c = scan[ jaxpr={ lambda  ; a b c.\n",
       "                        let d = div a b\n",
       "                            e = add b d\n",
       "                            f = mul e 0.5\n",
       "                        in (f,) }\n",
       "                length=10\n",
       "                linear=(False, False, False)\n",
       "                num_carry=1\n",
       "                num_consts=1\n",
       "                reverse=False\n",
       "                unroll=1 ] a 1.0 b\n",
       "  in (c,) }"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(scan_loop_sqrt)(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see how this translates in terms of gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let b = iota[ dimension=0\n",
       "                dtype=int32\n",
       "                shape=(10,) ] \n",
       "      _ _ c _ d =\n",
       "        scan[ jaxpr={ lambda  ; f a b c d e.\n",
       "                      let g = div f c\n",
       "                          h = add c g\n",
       "                          i = mul h 0.5\n",
       "                          j = integer_pow[ y=-2 ] c\n",
       "                      in (i, *, c, *, j) }\n",
       "              length=10\n",
       "              linear=(False, True, True, False, True, False)\n",
       "              num_carry=2\n",
       "              num_consts=3\n",
       "              reverse=False\n",
       "              unroll=1 ] a * * 1.0 * b\n",
       "      _ e _ _ _ =\n",
       "        scan[ jaxpr={ lambda  ; a b c d e f g.\n",
       "                      let h = mul e 0.5\n",
       "                          i = mul h g\n",
       "                          j = mul i a\n",
       "                          k = neg j\n",
       "                          l = add_any h k\n",
       "                          m = div h f\n",
       "                          n = add_any c m\n",
       "                      in (b, n, *, l, *) }\n",
       "              length=10\n",
       "              linear=(False, True, True, True, True, False, False)\n",
       "              num_carry=4\n",
       "              num_consts=1\n",
       "              reverse=True\n",
       "              unroll=1 ] a * 0.0 * 1.0 c d\n",
       "  in (e,) }"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(grad(scan_loop_sqrt))(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.5, dtype=float32), DeviceArray(1., dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jvp(scan_loop_sqrt, (0.25,), (1.,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.5, dtype=float32), DeviceArray(1., dtype=float32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jvp(while_loop_sqrt, (0.25,), (1.,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(1., dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(scan_loop_sqrt)(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFilteredStackTrace\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b45ec8a825bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhile_loop_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36m_while_transpose_error\u001b[0;34m(*_, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_while_transpose_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m   raise ValueError(\"Reverse-mode differentiation does not work for \"\n\u001b[0m\u001b[1;32m    539\u001b[0m                    \u001b[0;34m\"lax.while_loop or lax.fori_loop. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFilteredStackTrace\u001b[0m: ValueError: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead.\n\nThe stack trace above excludes JAX-internal frames.\nThe following is the original exception that occurred, unmodified.\n\n--------------------",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b45ec8a825bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhile_loop_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreraise_with_filtered_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_under_reraiser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/api.py\u001b[0m in \u001b[0;36mgrad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_grad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreraise_with_filtered_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_under_reraiser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/api.py\u001b[0m in \u001b[0;36mvalue_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_check_output_dtype_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholomorphic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/api.py\u001b[0m in \u001b[0;36m_vjp_pullback_wrapper\u001b[0;34m(cotangent_dtypes, io_tree, fun, py_args)\u001b[0m\n\u001b[1;32m   1791\u001b[0m              \"with dtype {}.\")\n\u001b[1;32m   1792\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_tangent_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m   \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36munbound_vjp\u001b[0;34m(pvals, jaxpr, consts, *cts)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mcts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_consts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mdummy_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mUndefinedPrimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0marg_cts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstantiate_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_cts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(jaxpr, consts, primals_in, cotangents_in)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         cts_out = get_primitive_transpose(eqn.primitive)(cts_in, *invals,\n\u001b[0;32m--> 221\u001b[0;31m                                                          **eqn.params)\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcts_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mZero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvars\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcts_out\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mZero\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcts_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# FIXME: Some invars correspond to primals!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/jax-workshop/venv/lib/python3.7/site-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36m_while_transpose_error\u001b[0;34m(*_, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_while_transpose_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m   raise ValueError(\"Reverse-mode differentiation does not work for \"\n\u001b[0m\u001b[1;32m    539\u001b[0m                    \u001b[0;34m\"lax.while_loop or lax.fori_loop. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m                    \"Try using lax.scan instead.\")\n",
      "\u001b[0;31mValueError\u001b[0m: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead."
     ]
    }
   ],
   "source": [
    "grad(while_loop_sqrt)(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the automatic differentiation, reverse-mode needs to allocate memory, which can't be done dynamically in XLA, so that JAX doesn't allow for reverse mode differentiation through while_loops which could grow indefinitely. Instead some work has been planned in JAX to allow for bounded size while loops to be implemented using the scan syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: \n",
    "Implement the following bubble sort algorithm:\n",
    "```python\n",
    "def bubbleSort(arr): \n",
    "    n = len(arr) \n",
    "    res = np.copy(arr)\n",
    "    for i in range(n-1): \n",
    "        for j in range(0, n-i-1): \n",
    "            if res[j] > res[j+1]: \n",
    "                res[j], res[j+1] = res[j+1], res[j]\n",
    "    return res   \n",
    "```\n",
    "What is its Jacobian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "Implement the following discrete [Hidden Markov Model](https://en.wikipedia.org/wiki/Hidden_Markov_model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_filter(A, B, pi, ys):\n",
    "    pxs = np.empty((ys.shape[0], pi.shape[0]))\n",
    "    for i, y in enumerat(ys):\n",
    "        B_y = B[:, y]  # likelihood\n",
    "        px = B_y * px  # unormalised bayes rule\n",
    "        px = px / px.sum()  # normalisation\n",
    "        pxs[i] = px  # registration\n",
    "        px = A @ px  # prediction\n",
    "    return pxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "### Prerequisites\n",
    "- Intermediate loops "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import make_jaxpr, jvp, vjp, jit\n",
    "from jax.lax import associative_scan, scan\n",
    "\n",
    "import jax.numpy as jax_np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now present an additional primitive which can be useful when dealing with associative binary operations (such as summation): `associative_scan`, also known as [prefix sum](https://en.wikipedia.org/wiki/Prefix_sum). It consists in applying recursive operations to subsets of the inputs (divide and conquer strategy) instead of applying it sequentially. This has the benefit of being easily parallelisable and is natively implemented in JAX. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice it implements a parallelised version (see for example the [Wikipedia](https://en.wikipedia.org/wiki/Prefix_sum) article, or ask Fatemeh, she's an expert at it) of the following algorithm:\n",
    "```python\n",
    "def my_sequential_associative_scan(binary_op, xs):\n",
    "    res = np.copy(xs)\n",
    "    val = xs[0]\n",
    "    for i, x in enumerate(xs[1:]):\n",
    "        val = binary_op(val, x)\n",
    "        res[i+1] = val\n",
    "    return res\n",
    "```\n",
    "\n",
    "so that the cumulative sum would be for example written as \n",
    "```python\n",
    "my_sequential_associative_scan(lambda x, y: x + y, np.arange(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using JAX this would actually be written in the following wa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.  6. 10. 15. 21. 28. 36. 45.]\n"
     ]
    }
   ],
   "source": [
    "def associative_cumulative_sum(xs):\n",
    "    return associative_scan(lambda x, y: x + y, xs)\n",
    "\n",
    "print(associative_cumulative_sum(np.arange(10.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the code generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let b = slice[ limit_indices=(9,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(2,) ] a\n",
       "      c = slice[ limit_indices=(10,)\n",
       "                 start_indices=(1,)\n",
       "                 strides=(2,) ] a\n",
       "      d = add b c\n",
       "      e = slice[ limit_indices=(4,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(2,) ] d\n",
       "      f = slice[ limit_indices=(5,)\n",
       "                 start_indices=(1,)\n",
       "                 strides=(2,) ] d\n",
       "      g = add e f\n",
       "      h = slice[ limit_indices=(1,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(2,) ] g\n",
       "      i = slice[ limit_indices=(2,)\n",
       "                 start_indices=(1,)\n",
       "                 strides=(2,) ] g\n",
       "      j = add h i\n",
       "      k = slice[ limit_indices=(0,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] j\n",
       "      l = slice[ limit_indices=(2,)\n",
       "                 start_indices=(2,)\n",
       "                 strides=(2,) ] g\n",
       "      m = add k l\n",
       "      n = slice[ limit_indices=(1,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] g\n",
       "      o = concatenate[ dimension=0 ] n m\n",
       "      p = pad[ padding_config=((0, 1, 1),) ] o 0.0\n",
       "      q = pad[ padding_config=((1, 0, 1),) ] j 0.0\n",
       "      r = add p q\n",
       "      s = slice[ limit_indices=(5,)\n",
       "                 start_indices=(2,)\n",
       "                 strides=(2,) ] d\n",
       "      t = add r s\n",
       "      u = slice[ limit_indices=(1,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] d\n",
       "      v = concatenate[ dimension=0 ] u t\n",
       "      w = pad[ padding_config=((0, 0, 1),) ] v 0.0\n",
       "      x = pad[ padding_config=((1, 1, 1),) ] r 0.0\n",
       "      y = add w x\n",
       "      z = slice[ limit_indices=(4,)\n",
       "                 start_indices=(0,)\n",
       "                 strides=(1,) ] y\n",
       "      ba = slice[ limit_indices=(10,)\n",
       "                  start_indices=(2,)\n",
       "                  strides=(2,) ] a\n",
       "      bb = add z ba\n",
       "      bc = slice[ limit_indices=(1,)\n",
       "                  start_indices=(0,)\n",
       "                  strides=(1,) ] a\n",
       "      bd = concatenate[ dimension=0 ] bc bb\n",
       "      be = pad[ padding_config=((0, 1, 1),) ] bd 0.0\n",
       "      bf = pad[ padding_config=((1, 0, 1),) ] y 0.0\n",
       "      bg = add be bf\n",
       "  in (bg,) }"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(associative_cumulative_sum)(np.arange(10.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks just as when we were doing python loops! Is it bad? No, because the depth of the generated graph will only grow in $\\log_2(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as for `scan`, it is closed under differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DeviceArray([ 0.,  1.,  3.,  6., 10.], dtype=float32), DeviceArray([1., 2., 3., 4., 5.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(jvp(associative_cumulative_sum, (np.arange(5.),), (np.ones(5),)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.  6. 10.]\n",
      "(DeviceArray([5., 4., 3., 2., 1.], dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "val, cumsum_bwd = vjp(associative_cumulative_sum, np.arange(5.))\n",
    "print(val)\n",
    "print(cumsum_bwd(np.ones(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: \n",
    "Compare the speed of the associative_scan and scan implementation of cumulative sum on GPU and CPU (use the device flag in the jit function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "Implements your own parallel version of `associative_scan` using jax primitives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-workshop",
   "language": "python",
   "name": "jax-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
